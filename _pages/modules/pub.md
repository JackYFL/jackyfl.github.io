# üìù Publications 
## üëÅÔ∏è Computer Vision

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">ArXiv</div><img src="./images/IndustryNav.jpg" alt="sym" width="100%"></div></div>
<div class="paper-box-text" markdown="1">

[ <br> IndustryNav: Exploring Spatial Reasoning of Embodied Agents in Dynamic Industrial Navigation](https://arxiv.org/pdf/2511.17384) \\
**Yifan Li**\*, Lichi Li\*, Anh Dao\*, Xinyu Zhou, Yicheng Qiao, Zheda Mai, Daeun Lee, Zichen Chen, Zhen Tan, Mohit Bansal, Yu Kong

[**Paper**](https://arxiv.org/pdf/2511.17384) /
[**Project**](https://jackyfl.github.io/IndustryEQA_project_page/)
</div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">ICCV 2025</div><img src="./images/vitsplit-framework.png" alt="sym" width="100%"></div></div>
<div class="paper-box-text" markdown="1">

[![Star](https://img.shields.io/github/stars/JackYFL/ViT-Split.svg?style=social&label=Star) <br> ViT-Split: Unleashing the Power of Vision Foundation Models via Efficient Splitting Heads](https://arxiv.org/pdf/2506.03433) \\
**Yifan Li**, Xin Li, Tianqin Li, Wenbin He, Yu Kong, Ren Liu

[**Paper**](https://arxiv.org/pdf/2506.03433) /
[**Code**](https://github.com/JackYFL/ViT-Split) /
[**Project**](https://jackyfl.github.io/vitsplit.github.io/)
</div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">NeurlPS 2025 D&B</div><img src="./images/IndustryEQA.png" alt="sym" width="100%"></div></div>
<div class="paper-box-text" markdown="1">

[![Star](https://img.shields.io/github/stars/JackYFL/IndustryEQA.svg?style=social&label=Star) <br>  IndustryEQA: Pushing the Frontiers of Embodied Question Answering in Industrial Scenarios](https://arxiv.org/pdf/2505.20640) \\
**Yifan Li**\*, Yuhang Chen\*, Anh Dao\*, Lichi Li, Zhongyi Cai, Zhen Tan, Tianlong Chen, Yu Kong

[**Paper**](https://arxiv.org/pdf/2505.20640) /
[**Project**](https://jackyfl.github.io/IndustryEQA_project_page/) /
[**Benchmark**](https://huggingface.co/datasets/IndustryEQA/IndustryEQA) /
[**Code**](https://github.com/JackYFL/IndustryEQA) 
</div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">CVPRW 2025</div><img src="./images/wico.png" alt="sym" width="100%"></div></div>
<div class="paper-box-text" markdown="1">

[![Star](https://img.shields.io/github/stars/JackYFL/WiCo.svg?style=social&label=Star) <br>  Window Token Concatenation for Efficient Visual Large Language Models](https://arxiv.org/pdf/2504.04024) \\
**Yifan Li**, Wentao Bao, Botao Ye, Zhen Tan, Tianlong Chen, Huan Liu, Yu Kong

[**Paper**](https://arxiv.org/pdf/2504.04024) /
[**Codes**](https://github.com/JackYFL/WiCo/) 
</div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">ArXiv</div><img src="./images/VLM_trends.png" alt="sym" width="100%"></div></div>
<div class="paper-box-text" markdown="1">

[![Star](https://img.shields.io/github/stars/JackYFL/awesome-VLLMs.svg?style=social&label=Star) <br> Visual Large Language Models for Generalized and Specilized Applications](https://arxiv.org/pdf/2501.02765) \\
**Yifan Li**, Zhixin Lai, Wentao Bao, Zhen Tan, Anh Dao, Kewei Sui, Jiayi Shen, Dong Liu, Huan Liu, Yu Kong

[**Paper**](https://arxiv.org/pdf/2501.02765) /
[**Project**](https://github.com/JackYFL/awesome-VLLMs) 
</div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">ECCV 2024</div><img src="./images/EmoLAv2_ECCV2024.png" alt="sym" width="100%"></div></div>
<div class="paper-box-text" markdown="1">

[![Star](https://img.shields.io/github/stars/JackYFL/EmoLA.svg?style=social&label=Star) <br> Facial Affective Behavior Analysis with Instruction Tuning](https://arxiv.org/pdf/2404.05052) \\
**Yifan Li**, Anh Dao, Wentao Bao, Zhen Tan, Tianlong Chen, Huan Liu, Yu Kong

[**Paper**](https://arxiv.org/pdf/2404.05052) /
[**Project**](https://jackyfl.github.io/FABA_project_page/) /
[**Code**](https://github.com/JackYFL/EmoLA) 
</div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">BMVC 2023</div><img src="./images/ReCoT_BMVC2023.png" alt="sym" width="100%"></div></div>
<div class="paper-box-text" markdown="1">

[ReCoT: Regularized Co-Training for Facial Action Unit Recognition with Noisy Labels](https://papers.bmvc2023.org/0102.pdf) \\
**Yifan Li**, Hu Han, Shiguang Shan, Zhilong Ji, Jinfeng Bai, Xilin Chen

[**Paper**](https://github.com/JackYFL/jackyfl.github.io/tree/main/files/BMVC2023_camera_readyV2.pdf) /
[**Appendix**](https://github.com/JackYFL/jackyfl.github.io/tree/main/files/BMVC2023_appendix.pdf) /
[**Poster**](https://github.com/JackYFL/jackyfl.github.io/tree/main/files/ReCoT_BMVC2023_poster.png)
</div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">CVPR 2023</div><img src="./images/DISC_CVPR2023.png" alt="sym" width="100%"></div></div>
<div class="paper-box-text" markdown="1">

[![Star](https://img.shields.io/github/stars/JackYFL/DISC.svg?style=social&label=Star) <br> DISC: Learning from Noisy Labels via Dynamic Instance-Specific Selection and Correction](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_DISC_Learning_From_Noisy_Labels_via_Dynamic_Instance-Specific_Selection_and_CVPR_2023_paper.pdf) \\
**Yifan Li**, Hu Han, Shiguang Shan, Xilin Chen

[**Paper**](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_DISC_Learning_From_Noisy_Labels_via_Dynamic_Instance-Specific_Selection_and_CVPR_2023_paper.pdf) / [**Code**](https://github.com/JackYFL/DISC) / [**Poster**](https://github.com/JackYFL/DISC/blob/main/assets/DISC_poster.png)
</div>
</div>


<div class="paper-box"><div class="paper-box-image"><div><div class="badge">ECCVW 2022</div><img src="./images/EMMA_ECCVW2022.png" alt="sym" width="100%"></div></div>
<div class="paper-box-text" markdown="1">

[![Star](https://img.shields.io/github/stars/JackYFL/EMMA_CoTEX_ABAW4.svg?style=social&label=Star) <br> Affective Behaviour Analysis Using Pretrained Model with Facial Prior](https://arxiv.org/pdf/2207.11679) \\
**Yifan Li**, Haomiao Sun, Zhaori Liu, Hu Han, Shiguang Shan

[**Paper**](https://arxiv.org/pdf/2207.11679) / [**Code**](https://github.com/JackYFL/EMMA_CoTEX_ABAW4) 

</div>
</div>

- ``CVPRW 2024`` [The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative](https://arxiv.org/pdf/2402.14859), Zhen Tan, Chengshuai Zhao, Raha Moraffah, **Yifan Li**, Yu Kong, Tianlong Chen, Huan Liu

## üìñ Natural Language Processing

- ``EMNLP 2025`` [Task-Aware Resolution Optimization for Visual Large Language Models](https://arxiv.org/pdf/2510.09822), Weiqing Luo, Zhen Tan, **Yifan Li**, Xinyu Zhao, Kwonjoon Lee, Behzad Dariush, Tianlong Chen

- ``PAKDD 2024`` [Smoa: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents](https://arxiv.org/pdf/2411.03284?), Dawei Li, Zhen Tan, Peijia Qian, **Yifan Li**, Kumar Satvik Chaudhary, Lijie Hu, Jiayi Shen

- ``EMNLP 2024`` ["Glue pizza and eat rocks" - Exploiting Vulnerabilities in Retrieval-Augmented Generative Models](https://arxiv.org/pdf/2406.19417), Zhen Tan, Chengshuang Zhao, RahaMoraffah, **Yifan Li**, Song Wang, Jundong Li, Tianlong Chen, Huan Liu

## üìö Other Research Papers

- ``TII 2020`` [An Indoor Localization Algorithm Based on Modified Joint Probabilistic Data Association for Wireless Sensor Network](https://ieeexplore.ieee.org/abstract/document/9031316), Long Cheng, **Yifan Li**, Mingkun Xue, Yan Wang
